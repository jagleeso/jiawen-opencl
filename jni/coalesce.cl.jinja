__kernel void coalesce_optimal(
        __global uint4 *src,
        uint elems)
{
    uint global_id = get_global_id(0);
    // 10. Set up __global memory access pattern.
    uint count = ( elems / 4 ) / get_global_size(0);
    /* CPU
     */
    // uint idx = get_global_id(0) * count;
    // uint stride = 1;
    /* GPU
     */
    uint idx = global_id;
    uint stride = get_global_size(0);

    /* Based on timing measurements, it looks like the OpenCL compiler will optimize 
     * away any reads that can't subsequently affect a write.  To work around this, make 
     * every read appear to matter by adding each zero entry to local variable pmin, and 
     * writing it only for the first work-item.
     */
    uint4 pmin;

    uint n;
    for (n = 0; n < count; n++, idx += stride) {
        pmin += src[idx];
    }

    if (global_id == 0) {
        src[0] = pmin;
    }

}

/* To work around the fact that we don't have the pragma unroll extension available to us, lets just use a 
 * templating engine to generate unrolled version of the function for us.
 */

/* Precondition:
 * elems is divisble by 4 * G * (spacing + 1).
 */
{% macro coalesce_spacing(spacing) -%}
{% if spacing is not none %}
__kernel void coalesce_spacing_{{spacing}}(
{% else %}
__kernel void coalesce_spacing(
{% endif %}
        __global uint4 *src,
        uint elems,
        uint spacing)
{
    uint global_id = get_global_id(0);
    uint G = get_global_size(0);

    uint4 pmin;

    uint n, i;
    uint count = (elems/4) / (G*(spacing + 1));
    uint idx = global_id*(spacing + 1);
    uint stride = G*(spacing + 1);
    for (n = 0; n < count; n++, idx += stride) {
        {% if spacing is not none %}

        {% for i in range(0, spacing + 1) -%}
        pmin += src[idx + {{i}}];
        {% endfor %}
        {% else %}
        for (i = 0; i < spacing + 1; i++) {
            pmin += src[idx + i];
        }
        {% endif %}
    }

    if (global_id == 0) {
        src[0] = pmin;
    }

}
{%- endmacro -%}

{% macro coalesce_spacing_group(spacing, group_size) -%}
/* Precondition:
 * elems is divisble by 4 * G * (spacing + 1) * {{group_size}}
 * (i.e. the work group size is {{group_size}}).
 */
__kernel void coalesce_spacing_group_{{spacing}}(
        __global uint4 *src,
        uint elems,
        uint spacing)
{
    uint global_id = get_global_id(0);
    uint G = get_global_size(0);

    uint4 pmin;

    uint n, i;
    uint count = (elems/4) / (G*(spacing + 1)*{{group_size}});
    uint idx = global_id*(spacing + 1);
    uint stride = G*(spacing + 1);
    for (n = 0; n < count; n++, idx += stride*{{group_size}}) {
        {% for i in range(0, group_size) -%}
        {% for j in range(0, spacing + 1) -%}
        pmin += src[idx + {{i}}*stride + {{j}}];
        {% endfor %}
        {% endfor %}
    }

    if (global_id == 0) {
        src[0] = pmin;
    }

}
{%- endmacro -%}


{% for i in range(0, config.MAX_SPACING + 1) %}
{{ coalesce_spacing(i) }}
{% endfor -%}

{{ coalesce_spacing(None) }}

{% if config.ENABLE_UNROLL_GROUP_MODE %}
{# {% for i in range(0, config.MAX_SPACING + 1) %} #}
{% for i in range(0, 3 + 1) %}
{{ coalesce_spacing_group(i, config.WORK_GROUP_SIZE) }}
{% endfor -%}
{% endif %}
