__kernel void coalesce_optimal(
        __global uint4 *src,
        uint elems)
{
    uint global_id = get_global_id(0);
    // 10. Set up __global memory access pattern.
    uint count = ( elems / 4 ) / get_global_size(0);
    /* CPU
     */
    // uint idx = get_global_id(0) * count;
    // uint stride = 1;
    /* GPU
     */
    uint idx = global_id;
    uint stride = get_global_size(0);

    /* Based on timing measurements, it looks like the OpenCL compiler will optimize 
     * away any reads that can't subsequently affect a write.  To work around this, make 
     * every read appear to matter by adding each zero entry to local variable pmin, and 
     * writing it only for the first work-item.
     */
    uint4 pmin;

    uint n;
    for (n = 0; n < count; n++, idx += stride) {
        pmin += src[idx];
    }

    if (global_id == 0) {
        src[0] = pmin;
    }

}

/* To work around the fact that we don't have the pragma unroll extension available to us, lets just use a 
 * templating engine to generate unrolled version of the function for us.
 */

/* Precondition:
 * elems is divisble by 4 * G * (spacing + 1).
 */
{% macro coalesce_spacing(spacing) -%}
__kernel void coalesce_spacing_{{spacing}}(
        __global uint4 *src,
        uint elems,
        uint spacing)
{
    uint global_id = get_global_id(0);
    uint G = get_global_size(0);

    uint4 pmin;

    uint n, i;
    uint count = (elems/4) / (G*(spacing + 1));
    uint idx = global_id*(spacing + 1);
    uint stride = G*(spacing + 1);
    for (n = 0; n < count; n++, idx += stride) {

        {% for i in range(0, spacing + 1) -%}
        pmin += src[idx + {{i}}];
        {% endfor %}
    }

    if (global_id == 0) {
        src[0] = pmin;
    }

}
{%- endmacro -%}

{% for i in range(0, config.MAX_SPACING) %}
{{ coalesce_spacing(i) }}
{% endfor %}
